{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务名称：协程控制任务简介：\n",
    "1.    深入理解协程的理论\n",
    "2.    掌握查看源码的方式\n",
    "3.    熟练掌握gevent模块的用法 \n",
    "社群话题：Python能够提升执行效率的协程是怎么样实现的 说明：           我们之前分别写过百度和腾讯的产品的简易版，而阿里作为三巨头的老大显然也不能错过，阿里比较有特点产品之一就是双十一淘宝天猫的高并发量，从2009年开始提出双十一的概念，最初的前几年为了实现大数据量的处理，一方面由阿里云CTO王坚带领团队研究云计算架构，另一面淘宝团队就在研究Nginx，力求把它的并发量发挥到极致，现在我们就来自己写一个吧。\n",
    "\n",
    "完成以下练习（上传代码链接）：      用尽毕生所学写一个自己Nginx，我们只需要写TCP协议的套接字服务端，客户端发送任何数据给服务端，服务端只需要回一个“Hello World”+ 客户端编号就可以了，\n",
    "比如：“Hello World1”,”Hello World2”就可以了，越快越好，要求如下：    \n",
    "1 首先确保数据安全，不论并发还是并行，回的内容不能出现重复的数字    \n",
    "2 能够从一定程度上抵御 SYN洪水攻击      \n",
    "3 使用池的概念，在机器硬件承受范围内自己加以控制      \n",
    "4 程序不能出现明显的阻塞        \n",
    "5 进程，线程，协程综合使用，至少能处理每秒百万的并发量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 那么首先你要明白，什么是协程？\n",
    "\n",
    "协程是实现并发编程的一种方式。一说并发，你肯定想到了多线程 / 多进程模型，没错，多线程 / 多进程，正是解决并发问题的经典模型之一。最初的互联网世界，多线程 / 多进程在服务器并发中，起到举足轻重的作用。\n",
    "\n",
    "随着互联网的快速发展，你逐渐遇到了 C10K 瓶颈，也就是同时连接到服务器的客户达到了一万个。于是很多代码跑崩了，进程上下文切换占用了大量的资源，线程也顶不住如此巨大的压力，这时， NGINX 带着事件循环出来拯救世界了。\n",
    "\n",
    "如果将多进程 / 多线程类比为起源于唐朝的藩镇割据，那么事件循环，就是宋朝加强的中央集权制。事件循环启动一个统一的调度器，让调度器来决定一个时刻去运行哪个任务，于是省却了多线程中启动线程、管理线程、同步锁等各种开销。同一时期的 NGINX，在高并发下能保持低资源低消耗高性能，相比 Apache 也支持更多的并发连接。\n",
    "\n",
    "再到后来，出现了一个很有名的名词，叫做回调地狱（callback hell），手撸过 JavaScript 的朋友肯定知道我在说什么。我们大家惊喜地发现，这种工具完美地继承了事件循环的优越性，同时还能提供 async / await 语法糖，解决了执行性和可读性共存的难题。于是，协程逐渐被更多人发现并看好，也有越来越多的人尝试用 Node.js 做起了后端开发。（讲个笑话，JavaScript 是一门编程语言。）\n",
    "\n",
    "回到我们的 Python。使用生成器，是 Python 2 开头的时代实现协程的老方法了，Python 3.7 提供了新的基于 asyncio 和 async / await 的方法。我们这节课，同样的，跟随时代，抛弃掉不容易理解、也不容易写的旧的基于生成器的方法，直接来讲新方法。\n",
    "\n",
    "我们先从一个爬虫实例出发，用清晰的讲解思路，带你结合实战来搞懂这个不算特别容易理解的概念。之后，我们再由浅入深，直击协程的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从一个爬虫说起\n",
    "爬虫，就是互联网的蜘蛛，在搜索引擎诞生之时，与其一同来到世上。爬虫每秒钟都会爬取大量的网页，提取关键信息后存储在数据库中，以便日后分析。爬虫有非常简单的 Python 十行代码实现，也有 Google 那样的全球分布式爬虫的上百万行代码，分布在内部上万台服务器上，对全世界的信息进行嗅探。\n",
    "\n",
    "话不多说，我们先看一个简单的爬虫例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（注意：本节的主要目的是协程的基础概念，因此我们简化爬虫的 scrawl_page 函数为休眠数秒，休眠时间取决于 url 最后的那个数字。）\n",
    "\n",
    "这是一个很简单的爬虫，main() 函数执行时，调取 crawl_page() 函数进行网络通信，经过若干秒等待后收到结果，然后执行下一个。\n",
    "\n",
    "看起来很简单，但你仔细一算，它也占用了不少时间，五个页面分别用了 1 秒到 4 秒的时间，加起来一共用了 10 秒。这显然效率低下，该怎么优化呢？\n",
    "\n",
    "于是，一个很简单的思路出现了——我们这种爬取操作，完全可以并发化。我们就来看看使用协程怎么写。\n",
    "\n",
    "最后，我们需要 asyncio.run 来触发运行。asyncio.run 这个函数是 Python 3.7 之后才有的特性，可以让 Python 的协程接口变得非常简单，你不用去理会事件循环怎么定义和怎么使用的问题（我们会在下面讲）。一个非常好的编程规范是，asyncio.run(main()) 作为主程序的入口函数，在程序运行周期内，只调用一次 asyncio.run。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "\n",
    "########## 输出 ##########\n",
    "\n",
    "crawling url_1\n",
    "OK url_1\n",
    "crawling url_2\n",
    "OK url_2\n",
    "crawling url_3\n",
    "OK url_3\n",
    "crawling url_4\n",
    "OK url_4\n",
    "Wall time: 10 s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，你就大概看懂了协程是怎么用的吧。不妨试着跑一下代码，欸，怎么还是 10 秒？\n",
    "\n",
    "10 秒就对了，还记得上面所说的，await 是同步调用，因此， crawl_page(url) 在当前的调用结束之前，是不会触发下一次调用的。于是，这个代码效果就和上面完全一样了，相当于我们用异步接口写了个同步代码。\n",
    "\n",
    "现在又该怎么办呢？\n",
    "\n",
    "其实很简单，也正是我接下来要讲的协程中的一个重要概念，任务（Task）。老规矩，先看代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "\n",
    "########## 输出 ##########\n",
    "\n",
    "crawling url_1\n",
    "crawling url_2\n",
    "crawling url_3\n",
    "crawling url_4\n",
    "OK url_1\n",
    "OK url_2\n",
    "OK url_3\n",
    "OK url_4\n",
    "Wall time: 3.99 s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以看到，我们有了协程对象后，便可以通过 asyncio.create_task 来创建任务。任务创建后很快就会被调度执行，这样，我们的代码也不会阻塞在任务这里。所以，我们要等所有任务都结束才行，用for task in tasks: await task 即可。\n",
    "\n",
    "这次，你就看到效果了吧，结果显示，运行总时长等于运行时间最长的爬虫。\n",
    "\n",
    "当然，你也可以想一想，这里用多线程应该怎么写？而如果需要爬取的页面有上万个又该怎么办呢？再对比下协程的写法，谁更清晰自是一目了然。\n",
    "\n",
    "其实，对于执行 tasks，还有另一种做法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "\n",
    "########## 输出 ##########\n",
    "\n",
    "crawling url_1\n",
    "crawling url_2\n",
    "crawling url_3\n",
    "crawling url_4\n",
    "OK url_1\n",
    "OK url_2\n",
    "OK url_3\n",
    "OK url_4\n",
    "Wall time: 4.01 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "狮子王 07月12日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2559742751.jpg\n",
      "命运之夜——天之杯II ：迷失之蝶 07月12日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561910374.jpg\n",
      "素人特工 07月12日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2560447448.jpg\n",
      "机动战士高达NT 07月12日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2558661806.jpg\n",
      "舞动吧！少年 07月12日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2555119986.jpg\n",
      "嘿，蠢贼 07月16日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2560832388.jpg\n",
      "银河补习班 07月18日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2561542089.jpg\n",
      "灰猴 07月18日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2561541477.jpg\n",
      "匠心 07月18日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2553935771.jpg\n",
      "刀背藏身 07月19日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2557644589.jpg\n",
      "猪八戒·传说 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561910053.jpg\n",
      "未来机器城 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561782665.jpg\n",
      "游戏人生 零 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561782374.jpg\n",
      "旺扎的雨靴 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561371801.jpg\n",
      "怨灵3：看不见的小孩 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561451573.jpg\n",
      "天池水怪 07月19日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561900220.jpg\n",
      "精灵小王子 07月20日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561172763.jpg\n",
      "心之山 07月20日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561899653.jpg\n",
      "桂香街 07月22日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2562011337.jpg\n",
      "隧道尽头 07月25日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2336413879.jpg\n",
      "小Q 07月25日 https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2561273340.jpg\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    init_page = requests.get(url).content\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        response_item = requests.get(url_to_fetch).content\n",
    "        soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "%time main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py:1377: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  self.tb = tb\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiohttp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3cf083014385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'aiohttp'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_content(url):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=header, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    init_page = await fetch_content(url)\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "%time asyncio.run(main())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "到这里，今天的主要内容就讲完了。今天我用了较长的篇幅，从一个简单的爬虫开始，到一个真正的爬虫结束，在中间穿插讲解了 Python 协程最新的基本概念和用法。这里带你简单复习一下。\n",
    "\n",
    "协程和多线程的区别，主要在于两点，一是协程为单线程；二是协程由用户决定，在哪些地方交出控制权，切换到下一个任务。\n",
    "协程的写法更加简洁清晰，把 async / await 语法和 create_task 结合来用，对于中小级别的并发需求已经毫无压力。\n",
    "写协程程序的时候，你的脑海中要有清晰的事件循环概念，知道程序在什么时候需要暂停、等待 I/O，什么时候需要一并执行到底。\n",
    "最后的最后，请一定不要轻易炫技。多线程模型也一定有其优点，一个真正牛逼的程序员，应该懂得，在什么时候用什么模型能达到工程上的最优，而不是自觉某个技术非常牛逼，所有项目创造条件也要上。技术是工程，而工程则是时间、资源、人力等纷繁复杂的事情的折衷。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
